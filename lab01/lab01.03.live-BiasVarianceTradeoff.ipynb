{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "<tr>\n",
    "    <td><h1 style=\"text-align: left; font-size:300%;\">\n",
    "        Bias-Variance Tradeoff\n",
    "    </h1></td>\n",
    "    <td width=\"20%\">\n",
    "    <div style=\"text-align: right\">\n",
    "    <b> Machine Learning 2021</b> <br>\n",
    "    <b>Lab01.03 - 17/03/2021<br>\n",
    "    Marco Cannici <br>\n",
    "    <a href=\"mailto:marco.cannici@polimi.it\">marco.cannici@polimi.it</a><br>\n",
    "    </div>\n",
    "    </td>\n",
    "    <td width=\"100px\"> \n",
    "        <a href=\"http://chrome.ws.dei.polimi.it/index.php?title=Machine_Learning_Bio\">\n",
    "        <img align=\"right\", width=\"100px\" src='https://chart.googleapis.com/chart?cht=qr&chl=chrome.ws.dei.polimi.it/index.php?title=Machine_Learning_Bio&chs=180x180&choe=UTF-8&chld=L|0' alt=''>\n",
    "        </a>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset and a parametric function to optimize, there are usually many different ML approaches that one can take to optimally solve the problem. However, in many cases, the most powerful model may not be the best solution for the task at hand. Choosing the right model to use and having a metric to measure the performance of a trained model are two key concepts in machine learning. \n",
    "\n",
    "In this notebook we will use a simple example to show some a fundamental problem in machine learning that relates to model complexity and data availability: the **bias-variance tradeoff**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning - Recap (Regression)\n",
    "\n",
    "When performing regression analysis we would like to model the relationship between a continuous variable $f(x) \\in \\mathbb{R}$ and some input variables $x_1, x_2, .., x_p \\in \\mathbb{R}$. In general, however, **we are not able to measure the true relationship between $x$ and $f(x)$** since:\n",
    "- we can only measure the response of the system through a noisy observation (e.g., a sensor providing inconsistent readings)\n",
    "- we are not able to exactly model all the phenomena (e.g., input variables) that may influence the system response (e.g., an earthwake monitor device may detect a train passing in nearby subway and generate noisy readings because of that; we cannot model a real environment perfectly)\n",
    "\n",
    "In general, we model this uncertanty in our observations as an additive random noise $\\epsilon$: $y = f(\\mathbf{x}) + \\epsilon$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^p$ is a vector collecting all input variables, $y \\in \\mathbb{R}$ is the observation, and $\\epsilon$ is usually distributed as $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "### Fitting a regression model\n",
    "\n",
    "Given a set of $N$ observations $(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), ..., (\\mathbf{x}_N, y_N)$ we want to fit a model $\\hat{y_i} = \\hat{f}(\\mathbf{x}_i)$ in such a way that the model prediction is as close as possible to the observed response $y_i$. That is, given an **error meause** $E$, we want to find the  parameters of the model that minimize $E(y_i, \\hat{y}_i)$ $\\forall i = 1,...,N$\n",
    "\n",
    "For regression models we usually measure the model performance in terms of the mean squared error $MSE(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{N} \\sum_{i=1}^N{(y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "### Training and test\n",
    "\n",
    "In order to measure the *true* performance of the model, we usually split the set of available data into the **training** and **test** sets. We use the first to tune the model parameters while the second one to measure its performance on data that has never seen during training.\n",
    "\n",
    "We distinguish between **training error**, the error computed on training data, and **test error**, the one computed on the leave-out set. \n",
    "\n",
    "### Reducible and irreducible error\n",
    "\n",
    "When training a model we have to remember that the performance of the model depends on two quantities, which we usually call **reducible** and **irreducible** errors. Since we can only observe the system through noisy observations, even the perfect model $\\hat{f}$, that has been trained on infinitely many training data and it is able to perfectly predict them, will have an error on a test set. The additive noise $\\epsilon$ is indeed independent from the set of variables $\\mathbf{x}$ we can observe and we have no way to minimize it. We call this error **irriducible** error, while the remaining part of the error (that depends on observable variables) the **reducible error**\n",
    "\n",
    "$\\mathbb{E}(y-\\hat{y})^{2} =\\mathbb{E}[f(\\mathbf{x})+\\epsilon-\\hat{f}(\\mathbf{x})]^{2}\n",
    "=\\underbrace{[f(\\mathbf{x})-\\hat{f}(\\mathbf{x})]^{2}}_{\\text {Reducible }}+\\underbrace{\\operatorname{Var}(\\epsilon)}_{\\text {Irreducible }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that \n",
    "- there exists an **unknown** relationship $f(x) = 2 \\cdot sin(1.5 \\cdot x)$ between an observed variable $x$ and an output $y$\n",
    "-  we are given $N$ (noisy) observation of such relationship $y = f(x) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "We want to fit a **regression model** (with a tunable complexity **degree**) to predict the output value $y$ given an obseration point $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_f(nsamples, minx=0.0, maxx=4.5):\n",
    "    \"\"\"\n",
    "    Return 'nsamples' samples without noise with x ranging\n",
    "    within the interval ['minx', 'maxx']\n",
    "    \n",
    "    Arguments:\n",
    "        nsamples (int): the number of samples to generate\n",
    "        minx (float): the minimum x value\n",
    "        maxx (float): the maximum x value\n",
    "    \"\"\"\n",
    "    # Generate 'nsampels' equally spaced points\n",
    "    # between the [minx, maxx] range\n",
    "    # x.shape = [nsamples]\n",
    "    x = #...\n",
    "    y = #...\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(nsamples, mu=0.0, var=1.0):\n",
    "    \"\"\"\n",
    "    Generate additive noise distributed as a Gaussian with mean 'mu'\n",
    "    and variance 'var'\n",
    "    \n",
    "    Arguments:\n",
    "        nsamples (int): the number of error values to generate\n",
    "        mu (float): the mean of the Gaussian distribution\n",
    "        var (float): the variance of the Gaussian distribution\n",
    "    \"\"\"\n",
    "    # Generate a noise value for each point \n",
    "    e = #...\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess the data so that we can use sklearn models\n",
    "    \"\"\"\n",
    "    \n",
    "    # We sort the points in increasing order of x so that\n",
    "    # using plt.plot() will result in a smooth function\n",
    "    \n",
    "    # WARNING: We do this operation here for convenience,\n",
    "    # and since the fitting procedure we will use does not \n",
    "    # depend on the sample order. However this is not always \n",
    "    # the case and some training procedure may benefit from \n",
    "    # random order!\n",
    "    idx_sorted = np.argsort(x)\n",
    "    x, y = x[idx_sorted], y[idx_sorted]\n",
    "    \n",
    "    # We add a dimension since sklearn wants [N, 1] input\n",
    "    # arrays, rather than [N] arrays\n",
    "    x = np.expand_dims(x, -1)\n",
    "    y = np.expand_dims(y, -1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 30\n",
    "np.random.seed(42)\n",
    "\n",
    "# Training dataset\n",
    "# ================\n",
    "# x.shape = [nsamples], f.shape = [nsamples]\n",
    "x, f = #...\n",
    "# We sample a noise value for each data point\n",
    "# in the training set, e.shape = [nsamples]\n",
    "e = #...\n",
    "\n",
    "y = #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perc = 0.2\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split training and test\n",
    "# =======================\n",
    "x_train, x_test, y_train, y_test = #...\n",
    "\n",
    "# Preprocess data\n",
    "# ===============\n",
    "# The library we use needs the input arrays to be\n",
    "# of shape [n_samples, n_features] while our arrays\n",
    "# are now of shape [n_samples]\n",
    "x_train, y_train = #...\n",
    "x_test, y_test = #...\n",
    "\n",
    "# Plot the data\n",
    "# =============\n",
    "# Plot points with different colors based on the set\n",
    "#...\n",
    "#...\n",
    "plt.legend()\n",
    "\n",
    "# Plot the true function f using a lot of points \n",
    "# (>> nsamples) in order to have a smooth function\n",
    "tmp_x, tmp_f = #...\n",
    "plt.plot(tmp_x, tmp_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the python package **scikit-learn** to fit a Linear model (with polynomial features of a certain degree). Since we still don't know how the model works and how it's trained (but you will soon!), in this notebook we will treat the model as a **black-box** function.\n",
    "\n",
    "Play with the degree parameter to see how the model complexity changes as the degree parameter increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(x, y, degree):\n",
    "    \"\"\"\n",
    "    Fit a polynomial model to the input data.\n",
    "    \n",
    "    Arguments:\n",
    "        x (np.ndarray): the input set of features as a numpy\n",
    "            array of shape [N]\n",
    "        y (np.ndarray): the target values as a numpy array\n",
    "            of shape [N]\n",
    "        degree (int): the polynomial degree\n",
    "            \n",
    "    Returns:\n",
    "        A polynomial model trained on the given training set\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Pipeline([('poly_features', \n",
    "                       PolynomialFeatures(degree=degree)), \n",
    "                      ('linear_regression', \n",
    "                       LinearRegression())])\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "# =============\n",
    "degree = 1\n",
    "# Fit the model (on the training set!!!)\n",
    "linear = #...\n",
    "# Evaluate the model on the same training data\n",
    "# to obtain the learnt function\n",
    "y_hat = #...\n",
    "\n",
    "# First plot the data points\n",
    "#...\n",
    "\n",
    "# Then the prediction surface\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance\n",
    "\n",
    "We measure the model performance using the mean squared error $MSE(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{N} \\sum_{i=1}^N{(y_i - \\hat{y}_i)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, y_hat):\n",
    "    \"\"\"\n",
    "    Computes the MLE between a target y and a predicted value y_hat\n",
    "    Note: this also equivalent to sklearn.metrics.mean_squared_error\n",
    "    \n",
    "    Arguments:\n",
    "        y (np.ndarray): An array containing target values for each data point\n",
    "        y_hat (np.ndarray): An array containing predicted values for each data point\n",
    "    \"\"\"\n",
    "    return #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = #...\n",
    "train_error = #...\n",
    "\n",
    "y_hat_test = #...\n",
    "test_error = #...\n",
    "\n",
    "print(\"Training error: {}\".format(train_error))\n",
    "print(\"Test error: {}\".format(test_error))\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# print(mean_squared_error(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model complexity as a function of degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the presence of noise and to the limited number of data points, fitting the most powerful model may not be the right choice, especially if the true (but unknown) reationship describing the data is simple, as in our case. By using a complex model we may indeed start \"to learn the noise in the data\", i.e., try to extract a relationship between $x$ and the noise that doesn't actually exist. \n",
    "\n",
    "We can visualize this behaviour by plotting the learnt models as a function of the model complexity (i.e., the polynomial degree in our case). As the degree increases (from light to intense colors) the model transitions from smooth functions to noisy functions that almost perfectly model the data (including noise!). Since the model gets more and more powerful, it starts to \"learn\" the training dataset we are using, rather than learning the general relationship between input and output values. This behaviour is usualy called **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 15)\n",
    "\n",
    "# First plot the data points\n",
    "plt.scatter(x_train, y_train, label=\"train\")\n",
    "plt.scatter(x_test, y_test, label=\"test\")\n",
    "\n",
    "# Setup colors \n",
    "cmap = plt.get_cmap('Reds')\n",
    "colors = cmap(np.linspace(0.2,1, len(degrees)))\n",
    "plt.gca().set_prop_cycle(cycler('color', colors))\n",
    "\n",
    "# Cycle through all degrees and train a model for\n",
    "# each degree. Then also keep track of all the\n",
    "# trained models within a list 'trained_models'\n",
    "trained_models = []\n",
    "for ...:\n",
    "    # Fit the model with a specific degree\n",
    "    #...\n",
    "    trained_models.append(model)\n",
    "    # Compute train predictions\n",
    "    #...\n",
    "    # Then plot the prediction surface\n",
    "    #...\n",
    "\n",
    "# Plot the true function\n",
    "plt.plot(tmp_x, tmp_f, \"b--\", label=\"True function\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test error as a function of degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the previous graphs, we expect two behaviours:\n",
    "- The **training error** (computed on training, blue, points) will **decrease** as the model complexity increases, since intense colored lines are more close to training points\n",
    "- The **test error** will initially decrease (very simple models are too simple) and then eventually start to increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and test performance\n",
    "# as a function of the model degree\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "for model in trained_models:\n",
    "    y_hat_train = #...\n",
    "    err_train = #...\n",
    "    train_errors.append(...)\n",
    "    \n",
    "    y_hat_test = #...\n",
    "    err_test = #...\n",
    "    test_errors.append(...)\n",
    "    \n",
    "min_idx = #...\n",
    "min_test_deg = #...\n",
    "min_test_err = #...\n",
    "\n",
    "plt.plot(degrees, train_errors, label=\"train\")\n",
    "plt.plot(degrees, test_errors, label=\"test\")\n",
    "plt.plot(min_test_deg, min_test_err, \"x\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a model we always search for a good compromise between model complexity and accuracy.\n",
    "\n",
    "Degree $3$ in this case is the best compromise since it provides the best performance on a set of never seen samples."
   ]
  },
  {
   "attachments": {
    "immagine.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance tradeoff - Recap\n",
    "\n",
    "This is a problem we have to face while training any machine learning model and it is closely related to the presence of *noise in the data* and the most often very *scarce availability of training pairs*. \n",
    "\n",
    "Most importantly, the U-shape of the test MSE curve turns out to be the result of *two competing properties of statistical learning methods*, i.e., the **bias** and **variance**, which are both related to the concept of *hypothesis space*.\n",
    "\n",
    "### Hypothesis space\n",
    "\n",
    "When we make the choice to solve a statistical problem using a specific class of models (e.g., polynomial functions of a specific fixed degree), we are effectively limiting the search space of the best model to a specific class of functions. We call the space of functions that can be represented by our model the **hypothesis space** $\\mathcal{H}$.\n",
    "\n",
    "![immagine.png](attachment:immagine.png)\n",
    "\n",
    "In our example, if we fix the polynomial degree to 1, we are effectively considering the *hypothesis space of lines*  and, by fitting the model, we are searching for the best line describing the data. The hypothesis space of lines will intuitively be quite **small** and probably **very far away** from the class of functions where the true function $f$ is actually contained (the class of sinusoidal functions)."
   ]
  },
  {
   "attachments": {
    "immagine.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance\n",
    "\n",
    "A model $\\hat{f}_{D_1}$ trained on the dataset $D_1$ is a particular point in the hypothesis space $\\mathcal{H}$. In other words, given a certain choice for the class of models, no matter the size of the training set, all the models $f_{D_i}$ will be inside the hypothesis space $\\mathcal{H}$.\n",
    "\n",
    "This implies that, if we choose a class of models that is not exactly the class of models of the target function (this is often the case when we model very complex behaviors), our predictions will always be a little bit different from the actual ones, no matter how well we fit the model. This concept (related to the error we commit by choosing a particular model), is called **bias**. We can think of the bias as the distance between the true function and the expected model, obtained by averaging models in $\\mathcal{H}$ trained on every possible dataset $D_i$ of a fixed size.\n",
    "\n",
    "Intuitively, the larger the hypothesis space, the larger the type (and complexity) of functions we can represent with our model class. However, choosing a large hypothesis space is usually connected with a greater variability of predictions. Indeed, since functions can have very different shapes in a large $\\mathcal{H}$, we may end up obtaining very different $\\hat{f}_{D_i}$ by training the *same model* on even slightly different datasets $D_i$. The tendence of a model to produce different estimations if trained with different datasets is called **variance**.\n",
    "\n",
    "![immagine.png](attachment:immagine.png)\n",
    "\n",
    "Intuitively, **the larger the $\\mathcal{H}$, the more complex/flexible the model and the larger the variance (connected to the hypothesis space size) but the lower the bias (connected to the distance from the true function $f$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize model bias and variance \n",
    "\n",
    "Let's try to fit the same model (fixed degree) with different datasets (of the same size) to have a first visual understanding on the concepts of bias and variance.\n",
    "\n",
    "*We expect simple models (low degrees) to produce functions that look all very similar and complex models (high degrees) to produce functions that have very different shapes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "ndatasets = 100\n",
    "nsamples = 30\n",
    "degree = 15\n",
    "var = 1.0\n",
    "\n",
    "# Fix the true distribution\n",
    "x_true, f_true = sample_f(nsamples)\n",
    "\n",
    "cmap = plt.get_cmap('Blues')\n",
    "colors = cmap(np.linspace(0,1, ndatasets))\n",
    "plt.gca().set_prop_cycle(cycler('color', colors))\n",
    "\n",
    "# For each x point, we keep track of the model response.\n",
    "# Each model will have the same complexity (flexibility) but\n",
    "# will be trained on a different training set that varies for\n",
    "# a small error\n",
    "y_sum = np.zeros([nsamples, 1])\n",
    "\n",
    "for i in range(ndatasets):\n",
    "    # Create a new dataset, i.e., add a \n",
    "    # Gaussian noise (different for each trial)\n",
    "    #...\n",
    "    # preprocess data\n",
    "    x, y = #...\n",
    "\n",
    "    # Fit the model and plot the train surface\n",
    "    # ...\n",
    "    \n",
    "    # Update the mean (we first sum all the points and then\n",
    "    # divide by the number of trials to obtain the mean)\n",
    "    y_sum #...\n",
    "\n",
    "y_mean = #...\n",
    "                          \n",
    "plt.plot(x_true, y_mean, color=\"yellow\", label=\"mean\")\n",
    "plt.plot(x_true, f_true, color=\"red\", label=\"true\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the cell with different **degree** values we observe that:\n",
    "\n",
    "- If we set the degree to low values (e.g., 1 to 4) we see that predicted blue functions look all very similar to each other, while they are very different if we set very high degree values (e.g., 10-15). Therefore, we expect the **variance** to be low for low degree values and high for high degrees.\n",
    "\n",
    "- On the contrary, if we look at the mean prediction (computed by averaging predictions produced for a point by all models $\\hat{y}_j = \\frac{1}{M} \\sum_{i=1}^{M}{\\hat{f}_{D_i}(x_j)}$) we see that it is very different from the true function for small degrees (simple models) but it looks almost the same as the true function for very high degree values. Therefore, we expect the **bias** to degrease as the degree increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing MSE bias and variance\n",
    "\n",
    "We know that, if we use the MSE error metric to measure the model performance, we can compute the model variance and bias with the following formula:\n",
    "\n",
    "${\\Large\\mathbb{E}}_{D_i}\\left[\\left(y_j - \\hat{f}_j^{D_i}\\right)\\right]$ = ${\\Large\\mathbb{E}}_{D_i}\\left[\\left(f_j + \\epsilon - \\hat{f}_j^{D_i}\\right) \\right] = \\underbrace{\\left( f_j - {\\Large\\mathbb{E}}_{D_i}\\left[\\hat{f}_j^{D_i}\\right] \\right)^2}_{bias^2} + {\\large Var}_{D_i}\\left[\\hat{f}_j^{D_i}\\right] + {\\large Var}[\\epsilon]$ \n",
    "\n",
    "where the expected value is computed over models fitted on different training datasets $D_i$ all of the same size. Notice: the formula provides the expected values of the MSE for a specific data point (i.e., the prediction for a fixed $x_j$); we have to averare over all points $x_j$ to obtain a single value representing the behavior over all the input domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bias_variance(degree, noise_var, nsamples, ndatasets):\n",
    "    \"\"\"\n",
    "    Compute the bias (squared) and variance for a polynomial model with a fixed degree\n",
    "    \n",
    "    Arguments:\n",
    "        degree (int): the degree of the polynomial\n",
    "        noise_var (float): the noise variance\n",
    "        nsamples (int): how many samples in each dataset\n",
    "        ndatasets (int): how many dataset to use to compute the expected value\n",
    "    \"\"\"\n",
    "    # Set the seed\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Fix the true distribution\n",
    "    x_true, f_true = sample_f(nsamples)\n",
    "    \n",
    "    y_hats = []\n",
    "    for i in range(ndatasets):\n",
    "        # Create a new dataset (different for each dataset)\n",
    "        # Add a Gaussian noise with noise_var\n",
    "        e = #...\n",
    "        y = #...\n",
    "        x, y = preprocess(x_true, y)\n",
    "\n",
    "        # fit the model\n",
    "        model = #...       \n",
    "        y_hat = #...\n",
    "        y_hats.append(y_hat)\n",
    "        \n",
    "    # From a list of ndatasets arrays of shape [nsamples, 1],\n",
    "    # to a [ndatasets, nsamples] array\n",
    "    y_hats = np.stack(y_hats, axis=0)[:,:,0]\n",
    "    y_mean = #...\n",
    "    # Bias[y_hat_0]^2 = (f_0 - E[y_hat_0])^2\n",
    "    bias2 = #...\n",
    "    # Var[y_hat_0]\n",
    "    var = #...\n",
    "    \n",
    "    return bias2, var\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degrees = range(1, 15)\n",
    "nsamples = 30\n",
    "ndatasets = 100\n",
    "noise_var = 1.0\n",
    "\n",
    "models_bias2 = []\n",
    "models_var = []\n",
    "\n",
    "# Compute the bias^2 and variance for each choice of degree\n",
    "for degree in degrees:\n",
    "    bias2, var = #...\n",
    "    models_bias2.append(bias2)\n",
    "    models_var.append(var)\n",
    "    \n",
    "noise_var = #... (a constant noise for each degree)\n",
    "tot_error = #...\n",
    "reducible_error = #...\n",
    "\n",
    "# Find best tradeoff using tot_error\n",
    "min_idx = #...\n",
    "min_err_x = #...\n",
    "min_err_y = #...\n",
    "    \n",
    "plt.plot(degrees, models_bias2, \"b\", label=\"bias[y_hat]^2\")\n",
    "plt.plot(degrees, models_var, \"g\", label=\"var[y_hat]\")\n",
    "plt.plot(degrees, noise_var, \"k--\", label=\"var[f]\")\n",
    "plt.plot(degrees, tot_error, \"r\", label=\"bias[y_hat]^2 + var[y_hat] + var[f]\")\n",
    "plt.plot(degrees, reducible_error, color=\"orange\", label=\"bias[y_hat]^2 + var[y_hat]\")\n",
    "plt.plot(min_err_x, min_err_y, \"rx\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we see that\n",
    "- the bias decreases as the complexity/flexibility increases\n",
    "- the variance increases as the complexity/flexibility increases\n",
    "\n",
    "Most importantly, we found that the best compromise for the bias/variance tradeoff is choosing a degree of $3$, which we already discovered by analyzing the MSE error on the testing set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
